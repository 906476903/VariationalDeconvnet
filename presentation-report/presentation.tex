\documentclass{beamer}

\mode<presentation> {

%\usetheme{default}
\usetheme{Rochester}
\usecolortheme{lily}

\setbeamertemplate{footline}[page number] 
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{bibliography item}{} %Remove icons in bibliography
}

\usepackage{graphicx} % Allows including images
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{listings}

\lstset{
    language=[5.0]Lua,
    basicstyle=\fontsize{11}{9},
    sensitive=true,
    breaklines=true,
    tabsize=2
}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Project Learning Systems]{Unsupervised training of Convolutional Networks with SGVB} 

\author{Joost van Amersfoort \& Otto Fabius} 
\institute[UvA] 
{University of Amsterdam
% Your institution for the title page
\medskip
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview}
\tableofcontents 
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\section{Introduction}


\begin{frame}
\frametitle{Introduction}
\begin{itemize}
	\item Convolutional Networks have had much success recently in Computer Vision tasks, due efficient implementation and an increase in computing power and available annotated data.
	\item However, to date, no successful means of training such Networks unsupervised has been reported.
	\item The recently developed SGVB makes efficient, effective Bayesian Inference possible by means of stochastic gradient descent. This can be used to train a ConvNet unsupervised
\end{itemize}
\end{frame}

\section{Theory}

\subsection{Convolutional Neural Networks}

\begin{frame}
\frametitle{Convolutional Neural Networks - Error Backpropagation}
The gradient of a convolution is relatively straightforward.\\ Let $L$ be the loss function, and $y = x * k$ the convolution operation with weights $W$, then \\   

\begin{align*}
\nabla_W L = (\nabla_y L) * x^T 
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Convolutional Neural Networks - Typical structure}

%image of example structure
%tell something about need for computational resources
\end{frame}

\subsection{SGVB}
\begin{frame}
\frametitle{Unsupervised Training with SGVB}
%image of example structure with latent variables and deconvolution
Model $P(\mathbf{X},\mathbf{Z}) = \frac{P(\mathbf{Z})\cdot P(\mathbf{X}|\mathbf{Z})}{P(\mathbf{Z}|\mathbf{X})}$ to find structure in data $\mathbf{X}$. \vspace{0.5mm}
Here, $P(\mathbf{X}|\mathbf{Z})$ is the deconvolutional layer and the intractable $P(\mathbf{Z}|\mathbf{X})$ is approximated by $q(\mathbf{Z}|\mathbf{X})$, which is the Convolutional layer.
%mention that we need to train the parameters of the network and that we can use this for pretraining an convnet, but also have a generative model which can be used for many different tasks.
\end{frame}

\begin{frame}
\frametitle{Marginal Likelihood as Optimization Criterion}
%Of course, we want to learn the parameters via gradient descent on some objective function
The marginal probability of $\mathbf{X}$ can be written as:
\begin{align*}
\log p_\theta(\mathbf{X}) = D_{KL}(q_\phi(\mathbf{Z}|\mathbf{X}) || p_\theta(\mathbf{Z}|\mathbf{X})) + \mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X})
\end{align*}
Where 
$\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X})$
is the \textit{lower bound} on the marginal likelihood
$
P(\mathbf{X}) = \int_z P(\mathbf{Z})P(\mathbf{X}|\mathbf{Z})d\mathbf{Z}$
\end{frame}

\begin{frame}
\frametitle{Reparameterization}
We need to estimate (stochastic) gradients w.r.t the model parameters. For this, we reparameterize $q(\mathbf{Z}|\mathbf{X})$ as a differentiable, deterministic function $g_{\theta}(\epsilon,\mathbf{x})$, which depends on sampled noise $\epsilon \sim N(0,1)$. \\ Now we can sample $\tilde{z} \sim q(\mathbf{Z}|\mathbf{X})$ and we can perform stochastic gradient descent on the model parameters!
\end{frame}

\section{Research Questions}

\section{Implementation}

\subsection{Torch7}
\begin{frame}
\frametitle{Implementation using \href{http://torch.ch/}{Torch7}}
\begin{itemize}
	\item Torch7 is a framework on top of Lua, utilizing the LuaJIT and crucial parts are in C/CUDA code
	\item Models in Torch7 consist of a stack of modules, e.g. Sigmoid, Linear and Convolution
	\item A module consists roughly of three functions:
		\begin{itemize}
			\item updateInput, which defines how the input is updated
			\item updateGradOutput, which defines how the gradient is updated during Backpropagation
			\item AccGradParameters, which defines how the parameters of the module are updated, given the gradient
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example of model in Torch7}
\begin{lstlisting}
require 'torch'
require 'nn'
model = nn.Sequential()

-- stage 1 : standard convolution layer
model:add(nn.SpatialConvolution(inputLayers, outputLayers, filterSizeW, filterSizeH))
model:add(nn.SpatialMaxPooling(windowH, windowW, strideH, strideW))
model:add(nn.Sigmoid())

-- stage 2 : standard 2-layer neural network
model:add(nn.Linear(256, 128))
model:add(nn.Sigmoid())
model:add(nn.Linear(128,#classes))
\end{lstlisting}
\end{frame}


\begin{frame}
\frametitle{Example of model in Torch7}

\begin{itemize}
	\item Output of model can be used as input for a criterion, a seperate module
	\item Criterion outputs the error (used for keeping track of lowerbound) and a gradient of the error w.r.t to the input
	\item What rests is a simple \texttt{model:forward(batch)} and \texttt{model:backward(batch, error)}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Using the GPU with Torch7}

\begin{itemize}
	\item Fairly straight forward process, as long as all modules support CUDA
	\item To get serious benefit, it was necessary to rewrite most code: instead of computation power, now memory is the scarce resource
	\item Important notes: necessary to use specific CUDA modules and single precision instead of double leads to numeric instability (log of very small numbers)
\end{itemize}

\end{frame}

 
\section{Results}

\section{Discussion}


%-----------------------------------------------
%\begin{frame}[shrink=20]{Bibliography}
%	\bibliographystyle{ieeetr}
%	\bibliography{ref}
%\end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{Thank you for your attention!}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 
