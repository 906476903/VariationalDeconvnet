In their paper, Kingma and Welling (ref) used a reparametrization technique to obtain unbiased, stochastic gradient to perform Variational Bayes. Here, the data $\mathbf{X}$ is assumed to be generated by some underlying random process which involves unobserved, or latent, continuous variables $\mathbf{z}$. The conditional probability $p(\mathbf{X}|\mathbf{z})$ and $q(\mathbf{z}|\mathbf{X})$ (the approximation of $p(\mathbf{z}|\mathbf{X})$) are modeled by fully-connected neural networks. These networks can be referred to as \textit{encoder} and \textit{decoder}, respectively, which can be nderstood by viewing the latent representation $\mathbf{z}$ of the data $\mathbf{X}$ as a code as in coding theory (ref). \\
These networks are trained using stochastic gradient descent on the lower bound:

\begin{align}
\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X}) = \log p_\theta(\mathbf{X}) - D_{KL}(q_\phi(\mathbf{z}|\mathbf{X}) || p_\theta(\mathbf{z}|\mathbf{X}))
\end{align}

In order to obtain estimates of the derivative of $\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X})$, $\mathbf{z}$ is sampled from the deterministic function $g_{\theta}(\epsilon,\mathbf{x})$, which depends on sampled noise $\epsilon \sim N(0,1)$. This is an effective, efficient method for unsupervised training of neural networks. Furthermore, the resulting model is generative, which comes with many advantages. \\
The approach we take in this project is very similar. The main difference is that in stead of using one or more fully connected layers to model $q(\mathbf{z}|\mathbf{X})$, convolutional layers are used. This means that for the decoder to model $p(\mathbf{X}|\mathbf{z})$ effectively, it needs to be capable of some form of deconvolution to reconstruct the data $\mathbf{X}$ from $\mathbf{z}$. 
