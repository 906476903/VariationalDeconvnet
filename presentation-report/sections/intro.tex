Convolutional Neural Networks (CNN's) are Neural Networks with convolutional layers, i.e. layers where the connections are such that the output of the layer corresponds to the convolution of the input with a set of learned weights. As in fully-connected Neural Networks, a point-wise non-linearity is applied after the convolution by means of an activation function. The upper limit of the performance of a CNN's is only slightly worse than that of fully-connected networks, but the weight-sharing nature of the connectivity results in much less parameters. \\
Recently, there has been much success in applying CNN's to image processing tasks such as classification (Krizhevsky), detection, and localization (OverFeat). This succes can mostly be attributed to an increase in the amount of available annotated data, the steady increase in computing power, and efficient implementations on GPU's. These CNN's are trained on a supervised error criterion, such as Binary Cross-Entropy on class labels for classification tasks (e.g. ...., OverFeat) or logistic regression on x-y coordinates for localization tasks (OverFeat).\\
However, so far there has only been success in the supervised training of such networks. Unsupervised training of CNN's, although desirable for a multitude of reasons (........ergens moet staan welke allemaal! Discussie of intro?...........), has not yet been done successfully. The most popular approach for unsupervised training of deep, fully-connected Neural Networks is layer-wise, by means of RBM's (Hinton refs ofzo). This technique is restricted to fully-connected layers and can therefore not be applied to CNN's. Recently, however, Kingma and Welling developed an effective, efficient method to perform approximate inference with Variational Bayes by means of stochastic gradient ascent on the variational lower bound. Stochastic Gradient Variational Bayes (SGVB) has been successfully applied to unsupervised training of fully-connected neural networks (Kingma and Welling), and first empirical results show that, for the tasks investigated, this outperforms RBM's (internal communication). \\
In the current work, we will make the first steps in investigating whether and how SGVB can be used with a CNN as encoding network. Since this method requires both an encoding network and a (generative (moet dit hier of bij volledige network??) decoding network (ref durk?), we will refer to the full model as a Deconvolutional Neural Network (DNN). (....hier past nog wel een link naar zeilers deconvnet?)


\subsection{This Project}
Firstly, we will briefly describe the framework we use to implement the DNN(s), as this is relevant for the choice network architectures. Next, we will describe the experiments we ran to answer our research questions. As DNN's have never been trained before (nb zeiler), our first question is how to perform deconvolution, i.e. what network architecture to use for the decoder. Next, we will investigate what network architectures can be trained efficiently, and how effective they are. Where appropriate, we will compare the results to similar, fully connected models. Finally, we discuss the results and make recommendations for further research on this topic.
\newpage



