Convolutional Neural Networks (CNN's) are Neural Networks with convolutional layers, i.e. layers where the connections are such that the output of the layer corresponds to the convolution of the input with a set of learned weights. As in fully-connected Neural Networks, a point-wise non-linearity is applied after the convolution by means of an activation function. The upper limit of the performance of a CNN's is only slightly worse than that of fully-connected networks, but the weight-sharing nature of the connectivity results in much less parameters. \\
Recently, there has been much success in applying CNN's to image processing tasks such as classification (Krizhevsky), detection, and localization (OverFeat). This succes can mostly be attributed to an increase in the amount of available annotated data, the steady increase in computing power, and efficient implementations on GPU's. These CNN's are trained on a supervised error criterion, such as Binary Cross-Entropy on class labels for classification tasks (e.g. ...., OverFeat) or logistic regression on x-y coordinates for localization tasks (OverFeat).\\
However, so far there has only been success in the supervised training of such networks. Unsupervised training of CNN's, although desirable for a multitude of reasons (........ergens moet staan welke allemaal! Discussie of intro?...........), has not yet been done successfully. The most popular approach for unsupervised training of deep, fully-connected Neural Networks is layer-wise, by means of RBM's (Hinton refs ofzo). This technique is restricted to fully-connected layers and can therefore not be applied to CNN's. Recently, however, Kingma and Welling developed an effective, efficient method to perform approximate inference with Variational Bayes by means of stochastic gradient ascent on the variational lower bound. Stochastic Gradient Variational Bayes (SGVB) has been successfully applied to unsupervised training of fully-connected neural networks (Kingma and Welling), and first empirical results show that, for the tasks investigated, this outperforms RBM's (internal communication). \\


\subsection{DNN's with SGVB}

In their paper, Kingma and Welling (ref) used a reparametrization technique to obtain unbiased, stochastic gradient to perform Variational Bayes. Here, the data $\mathbf{X}$ is assumed to be generated by some underlying random process which involves unobserved, or latent, continuous variables $\mathbf{z}$. The conditional probability $p(\mathbf{X}|\mathbf{z})$ and $q(\mathbf{z}|\mathbf{X})$ (the approximation of $p(\mathbf{z}|\mathbf{X})$) are modeled by fully-connected neural networks. These networks can be referred to as \textit{encoder} and \textit{decoder}, respectively, which can be understood by viewing the latent representation $\mathbf{z}$ of the data $\mathbf{X}$ as a code as in coding theory (ref). \\
These networks are trained using stochastic gradient descent on the lower bound:

\begin{align}
\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X}) = \log p_\theta(\mathbf{X}) - D_{KL}(q_\phi(\mathbf{z}|\mathbf{X}) || p_\theta(\mathbf{z}|\mathbf{X}))
\end{align}

In order to obtain estimates of the derivative of $\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{X})$, $\mathbf{z}$ is sampled from the deterministic function $g_{\theta}(\epsilon,\mathbf{x})$, which depends on sampled noise $\epsilon \sim N(0,1)$. This is an effective, efficient method for unsupervised training of neural networks. Furthermore, the resulting model is generative, which comes with many advantages. \\
The approach we take in this project is very similar. The main difference is that in stead of using one or more fully connected layers to model $q(\mathbf{z}|\mathbf{X})$, convolutional layers are used. This means that for the decoder to model $p(\mathbf{X}|\mathbf{z})$ effectively, it needs to be capable of some form of deconvolution to reconstruct the data $\mathbf{X}$ from $\mathbf{z}$. If successful, the resulting convolutional encoder and deconvolutional decoder can be used for many different tasks. From here on, we will refer to such model, in its entirety, as a Generative Convolutional Auto-Encoder (GCAE).

\subsection{Approach}
Firstly, we will introduce the framework we use to implement the DNN(s), as this is relevant for the choice network architectures. Hereafter we will describe the experiments we ran to answer our research questions. As GCAE's have never been trained before our first question is how to perform deconvolution, i.e. what network architecture to use for the decoder. Next, we will investigate what network architectures can be trained efficiently, and how effective they are. Where appropriate, we will compare the results to similar, fully connected models. Finally, we discuss the results and make recommendations for further research on this topic.
\newpage



